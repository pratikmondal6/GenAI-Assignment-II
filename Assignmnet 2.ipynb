{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhQdWO_4qqe"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <h1></h1>\n",
    "  <h1>Stylized Retrieval-Augmented Generation</h1>\n",
    "  <h4 align=\"center\">Assignmnet II</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi69xdNwKZPv"
   },
   "source": [
    "Welcome to Assignment II! In this notebook, you will build and implement a Retrieval-Augmented Generation (RAG) pipeline tailored for a text style transfer application.\n",
    "\n",
    "**By the end of this assignment, you'll be able to:**\n",
    "\n",
    "*   Build a Retrieval-Augmented Generation (RAG) pipeline to enhance text generation with external knowledge.\n",
    "*   Retrieve relevant information from a dataset or knowledge base to support text generation.\n",
    "*   Implement a neural style transfer model to transform text into a desired writing style.\n",
    "*   Combine retrieved content and style transfer to create a coherent and stylistically customized output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUWduwxuKgBa"
   },
   "source": [
    "## Important Note on Submission\n",
    "\n",
    "\n",
    "*   Do not use ChatGPT or any other AI tool to directly produce the code. If you need assistance, refer to Exercise 5 for guidance.\n",
    "*   You are allowed to work in a group of up to 3 members.\n",
    "*   Do not copy code or answers from other groups. Collaboration is encouraged only within your own group.\n",
    "*   Ensure that your notebook is runnable without any errors. Submissions with errors will not be accepted.\n",
    "*   Answers to open-ended questions must be original and not copied from other groups or AI tools like ChatGPT.\n",
    "*   The submission should be one .ipynb notebook with the group members' names on Openlat and matriculation numbers on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_gXYEhB7WzW"
   },
   "source": [
    "## Group Members\n",
    "\n",
    "\n",
    "1. First memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:\n",
    "2. Second memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:\n",
    "2. Third memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bss2qVkJKkZr"
   },
   "source": [
    "### Table of Contents\n",
    "- [1. Access to Hugging Face](#1-access-to-hugging-face)\n",
    "- [2. Packages](#2-packages)\n",
    "- [3. Problem Statement](#3-problem-statement)\n",
    "- [4. Fetch and Parse](#4-fetch-and-parse)\n",
    "- [5. Calculate Word Stats](#5-calculate-word-stats)\n",
    "- [6. Set Up LLM](#6-set-up-llm)\n",
    "- [7. BM25 Retriever](#7-bm25-retriever)\n",
    "- [8. Build Chroma](#8-build-chroma)\n",
    "- [9. Ensemble Retriever](#9-ensemble-retriever)\n",
    "- [10. Format Documents](#10-format-documents)\n",
    "- [11. RAG Chain](#11-rag-chain)\n",
    "- [12. Final Response](#12-final-response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZWrcJ4LKneE"
   },
   "source": [
    "# 1. Access to Hugging face\n",
    "Execute the following cell to connect to your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Pe3-F-kiKYiD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACEHUB_API_TOKEN: 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\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "print(\"HUGGINGFACEHUB_API_TOKEN:\", os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "#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\n",
    "\n",
    "# Prompt user for Hugging Face API token if not already set\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API token: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N72UZSMKqSa"
   },
   "source": [
    "# 2. Packages\n",
    "Execute the following code cells for installing the packages needed for creating your Stylized RAG.\n",
    "\n",
    "note: If there are package conflics you can use pip-tools to automatically find and install the compatible versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WLei89zCKsEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain || echo \"Error installing langchain\"\n",
    "!pip install -q langchain-community || echo \"Error installing langchain-community\"\n",
    "!pip install -q langchain-chroma || echo \"Error installing langchain-chroma\"\n",
    "!pip install -q langchain-huggingface || echo \"Error installing langchain-huggingface\"\n",
    "!pip install -q beautifulsoup4 || echo \"Error installing beautifulsoup4\"\n",
    "!pip install -q rank_bm25 || echo \"Error installing rank_bm25\"\n",
    "!pip install -q huggingface_hub || echo \"Error installing huggingface_hub\"\n",
    "!pip install -q requests || echo \"Error installing requests\"\n",
    "\n",
    "import langchain, langchain_community, langchain_chroma, langchain_huggingface, bs4, rank_bm25, huggingface_hub, requests; \n",
    "print(\"All packages imported successfully!\")\n",
    "\n",
    "\n",
    "# !pip show langchain langchain-community langchain-chroma langchain-huggingface beautifulsoup4 rank_bm25 huggingface_hub requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2K3p5lXKuSy"
   },
   "source": [
    "# 3. Problem Statement\n",
    "In this assignment, we will implement **Text Style Transfer**, a technique that modifies text style while preserving its content. They will build an **ensemble retriever** combining **BM25** for keyword-based retrieval and **Chroma** for semantic search to retrieve relevant documents, which will be used as input for the style transfer process. This project integrates classical retrieval methods with modern neural embeddings for practical NLP applications.\n",
    "\n",
    "**what is text style transfer?**\n",
    "\n",
    "**Text Style Transfer** is a natural language processing (NLP) technique that modifies the style of a given text while preserving its original content. It allows for the transformation of linguistic expressions to convey different tones, emotions, or writing styles without altering the underlying meaning. For example, it can rephrase formal text into a casual tone, adapt neutral statements into an emotional tone, or convert modern language into a Shakespearean style. This technique has applications in personalized communication, creative writing, sentiment adjustment, and even domain adaptation, making it a powerful tool for generating diverse textual outputs tailored to specific needs.\n",
    "\n",
    "### Example of Text Style Transfer:\n",
    "\n",
    "#### **Input (Neutral Tone):**\n",
    "\"I am excited about the opportunity to work on this project.\"\n",
    "\n",
    "#### **Output (Formal Tone):**\n",
    "\"I am genuinely enthusiastic about the prospect of contributing to this project.\"\n",
    "\n",
    "#### **Output (Casual Tone):**\n",
    "\"I'm super pumped to get started on this project!\"\n",
    "\n",
    "#### **Output (Shakespearean Style):**\n",
    "\"Verily, I am thrilled by the chance to partake in this noble endeavor.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQc6rEV4MHfY"
   },
   "source": [
    "# 4. Fetch and Parse\n",
    "In this part of the assignment, you are tasked with:\n",
    "\n",
    "*    Fetching and parsing web content: Write a function that fetches the HTML content of a webpage and processes it to extract clean, readable text.\n",
    "*    Splitting text into smaller chunks: Implement a function to split the text into overlapping chunks, ensuring that each chunk is manageable for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0kxayMljLPhu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters of fetched text:\n",
      "Machine learning - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us Contribute HelpLearn to editCommunity portalRecent changesUpload file Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more ContributionsTalk Contents move to sidebar hide (Top) 1 History 2 Relationships to other fields Toggle Relationships to othe\n",
      "Document 1:\n",
      "Machine learning - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us Contribute HelpLearn to editCommunity portalRecent changesUpload file Search Search Appearance Donate Create account Log in Personal\n",
      "Metadata: {'start': 0, 'end': 1000}\n",
      "\n",
      "Document 2:\n",
      " dictionary learning 4.6.4 Anomaly detection 4.6.5 Robot learning 4.6.6 Association rules 5 Models Toggle Models subsection 5.1 Artificial neural networks 5.2 Decision trees 5.3 Support-vector machines 5.4 Regression analysis 5.5 Bayesian networks 5.6 Gaussian processes 5.7 Genetic algorithms 5.8 Be\n",
      "Metadata: {'start': 900, 'end': 1900}\n",
      "\n",
      "Document 3:\n",
      "s 14 See also 15 References 16 Sources 17 Further reading 18 External links Toggle the table of contents Machine learning 87 languages Afrikaansالعربيةঅসমীয়াAzərbaycancaتۆرکجهবাংলা閩南語 / Bân-lâm-gúБашҡортсаБеларускаяभोजपुरीБългарскиབོད་ཡིགBosanskiCatalàČeštinaCymraegDanskالدارجةDeutschEestiΕλληνικάE\n",
      "Metadata: {'start': 1800, 'end': 2800}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "\n",
    "def fetch_and_parse(url: str) -> str:\n",
    "    try:\n",
    "        #Step 1 Taking the content from the url\n",
    "        responseFromUrl = requests.get(url)\n",
    "        responseFromUrl.raise_for_status() # Raise an error for failed requests\n",
    "\n",
    "        # Step 2: Parse the HTML content using BeautifulSoup.\n",
    "        pageContent = BeautifulSoup(responseFromUrl.text, 'html.parser')\n",
    "        \n",
    "        # Step 3: Extract the text content from the parsed HTML.\n",
    "        # Remove script, style, and other non-text content\n",
    "        for htmlTag in pageContent(['script', 'style', 'noscript']):\n",
    "            htmlTag.decompose()\n",
    "\n",
    "        # Get the raw text from the webpage\n",
    "        rawText = pageContent.get_text()\n",
    "\n",
    "        # Step 4: Clean the extracted text\n",
    "        Text_Without_WhiteSpace = ' '.join(rawText.split())  # Remove extra whitespace\n",
    "        return Text_Without_WhiteSpace\n",
    "        \n",
    "    except Exception as e:\n",
    "                         print(f\"Error fetching or parsing content from URL '{url}': {e}\")\n",
    "                         return \"\"\n",
    "def split_text_into_documents(text: str, chunk_size: int = 1000, overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks and return them as a list of Documents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The long text to split.\n",
    "    - chunk_size (int): The size of each chunk (default is 1000 characters).\n",
    "    - overlap (int): The number of overlapping characters between consecutive chunks (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of Documents, each containing a chunk of text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure chunk_size is greater than overlap\n",
    "        if chunk_size <= overlap:\n",
    "            raise ValueError(\"Chunk size must be greater than overlap.\")\n",
    "\n",
    "        # Initialize an empty list to store the chunks.\n",
    "        docs = []\n",
    "\n",
    "        # Split text into chunks with overlap\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + chunk_size, len(text))\n",
    "            chunk = text[start:end]\n",
    "            metadata = {\"start\": start, \"end\": end}  # Metadata about the chunk\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting text into documents: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fetch and parse text from a sample webpage\n",
    "    url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    parsedText = fetch_and_parse(url)\n",
    "    print(f\"First 500 characters of fetched text:\\n{parsedText[:500]}\")\n",
    "\n",
    "    # Split the text into documents\n",
    "    documentsAfterSplit = split_text_into_documents(parsedText, chunk_size=1000, overlap=100)\n",
    "    for i, document in enumerate(documentsAfterSplit[:3]):  # Print the first 3 chunks\n",
    "        print(f\"Document {i+1}:\\n{document.page_content[:300]}\\nMetadata: {document.metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4W_opcOi1o"
   },
   "source": [
    "1. Why do we split the text into smaller chunks before storing or processing it?\n",
    "\n",
    "Answer: Because it can help us to handle large texts easily and it can improve efficiency in terms of memory management, API limits, Token limits, etc. Also, it improves context retention with overlapping chunks, and search and retrieval accuracy. It also enables parallel processing. It helps associate metadata with chunks and ensures compatibility with fixed-size model inputs.\n",
    "2. What challenges might you face when fetching and parsing web content, and how would you handle them?\n",
    "\n",
    "Answer: In the case of large pages, it can take more time and memory to process. So, we can stream the response and parse it incrementally to minimize memory usage. Irregular HTML or poorly structured HTML is a problem for processing data therefore we can use robust libraries like BeautifulSoup. Some websites have anti-scrapping mechanisms to prevent that we can implement request throttling, use proxies or rotating IPs, and ensure compliance with the site's terms of service.\n",
    "\n",
    "3. In the context of RAG, how would errors in the fetch_and_parse function affect the overall pipeline?\n",
    "\n",
    "Answer: Errors in fetch_and_parse can affect the RAG pipeline by reducing the quality of the data, slowing down the response time also it can create a deadlock in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmZQ8HhGPugo"
   },
   "source": [
    "# 5. Calculate Word Stats\n",
    "\n",
    "In this task, you will implement a function to calculate basic word and character statistics for a list of documents. Each document is represented as a Document object with a page_content attribute that contains its text.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Calculate the total number of words and characters across all documents.\n",
    "2. Compute the average number of words and characters per document.\n",
    "3. Print the average statistics in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BkmqrEeYM-8z"
   },
   "outputs": [],
   "source": [
    "def calculate_word_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate and display average word and character statistics for a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): A list of Document objects, where each Document contains a `page_content` attribute.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the average word and character counts per document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize variables to keep track of total words and total characters.\n",
    "    total_words, total_characters = 0, 0\n",
    "\n",
    "    # Step 2: Iterate through each document in the `texts` list.\n",
    "    for doc in texts:\n",
    "        # Hint: `doc.page_content` contains the text of the document.\n",
    "        texts = doc.page_content\n",
    "        total_words += len(texts.split())\n",
    "        total_characters += len(texts)\n",
    "\n",
    "    # Step 3: Calculate the average words and characters per document.\n",
    "    # - Avoid division by zero by checking if the `texts` list is not empty.\n",
    "    num_of_documents = len(texts)\n",
    "    if num_of_documents > 0:\n",
    "        avg_words = total_words / num_of_documents\n",
    "        avg_characters = total_characters / num_of_documents\n",
    "    else:\n",
    "        avg_words = 0\n",
    "        avg_characters = 0\n",
    "\n",
    "    # Step 4: Print the calculated averages in a readable format.\n",
    "    # Example: \"Average words per document: 123.45\"\n",
    "    print(f\"Average words per document: {avg_words:.2f}\")\n",
    "    print(f\"Average characters per document: {avg_characters:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EpNcNKHdh_ar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 0.34\n",
      "Average characters per document: 1.98\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to test your calculate_word_stats function.\n",
    "# Create sample Document objects with text content for testing your code above.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"This is the first test document.\"),\n",
    "    Document(page_content=\"Here is another example document for testing.\"),\n",
    "    Document(page_content=\"Short text.\"),\n",
    "    Document(page_content=\"This document has more content. It's longer and has more words in it for testing purposes.\"),\n",
    "]\n",
    "\n",
    "# Call the function with the sample documents to calculate word statistics.\n",
    "calculate_word_stats(sample_docs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6zzYkovNisNP"
   },
   "source": [
    "1. What potential issues could arise if the texts list is empty or contains documents with no content, and how would you address them?\n",
    "\n",
    "Answer: \n",
    "If the list is empty then it will produce error as it has to divide by zero, the solution is add simple if logic.\n",
    "        if not texts:\n",
    "            print(\"No documents to process.\")\n",
    "            return\n",
    "\n",
    "For second problem, if one or more document has no content, means page_content is emtpy then it will give wrong answer for average. Just add a condition about not empty will solve this problem.\n",
    "\n",
    "\n",
    "2. Why is it beneficial to calculate both word count and character count instead of just one of them?\n",
    "\n",
    "Answer:  Words represent content quantity on the other hand character helps about formatting (limits and density).\n",
    "Both together is helpful for formatting and quality of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIIdfz7tlPdG"
   },
   "source": [
    "# 6. Set Up LLM\n",
    "\n",
    "In this part of the assignment, you will implement a function to set up a Large Language Model (LLM) using the Hugging Face Endpoint API. This function will:\n",
    "\n",
    "1. Initialize and connect to a pre-trained model available on Hugging Face.\n",
    "2. Allow customization of parameters like the model repository ID and generation temperature.\n",
    "3. Return the configured LLM object, which will be used later for text generation tasks in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnwtA4xolKWt"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def setup_llm(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Set up and return a Hugging Face LLM using the specified model repository ID and generation parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - repo_id (str): The repository ID of the Hugging Face model to use (default: \"mistralai/Mistral-7B-Instruct-v0.3\").\n",
    "    - temperature (float): The generation temperature to control creativity in outputs (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "    - HuggingFaceEndpoint: A configured LLM object ready for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Import the HuggingFaceEndpoint class.\n",
    "    # - This class allows you to connect to a Hugging Face model hosted on an endpoint.\n",
    "\n",
    "    # Step 2: Configure the LLM connection.\n",
    "    # - Use the HuggingFaceEndpoint class to set up the LLM.\n",
    "\n",
    "    # Step 3: Return the configured LLM object.\n",
    "    # - The returned LLM can be used for generating text based on input prompts.\n",
    "\n",
    "    # Write your code here.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQu0sETym99k"
   },
   "source": [
    "1. What would happen if the temperature is set to an extreme value (e.g., 0 or 10)? How would you prevent misuse?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If the LLM generates incorrect or irrelevant responses, what steps would you take to diagnose and fix the issue?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdEyYy86oxn9"
   },
   "source": [
    "# 7. BM25 Retriever\n",
    "\n",
    "In this task, students will implement a BM25 Retriever, a critical component of the RAG pipeline.\n",
    "Your task is to:\n",
    "\n",
    "1. Initialize the BM25 retriever with a set of documents.\n",
    "2. Implement a method to retrieve the top k most relevant documents for a given query.\n",
    "3. Use efficient tokenization and scoring to ensure accurate and fast results.\n",
    "This component will enable the pipeline to fetch relevant information from a corpus, which is then passed to the LLM for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp7Th24kmKLm"
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    A class to implement BM25-based document retrieval.\n",
    "\n",
    "    Attributes:\n",
    "    - documents (list): A list of Document objects.\n",
    "    - corpus (list): A list of strings representing the document contents.\n",
    "    - tokenized_corpus (list): A list of tokenized documents (lists of words).\n",
    "    - bm25 (BM25Okapi): The BM25 retriever initialized with the tokenized corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 retriever with the given documents.\n",
    "\n",
    "        Parameters:\n",
    "        - documents (list): A list of Document objects.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the input documents.\n",
    "        self.documents = documents\n",
    "        # Hint: Use the `page_content` attribute of each Document object to extract text.\n",
    "\n",
    "        # Step 2: Tokenize the corpus.\n",
    "        # Hint: Use the `.split()` method to tokenize each document into words.\n",
    "        self.corpus = [doc.page_content for doc in documents]\n",
    "        self.tokenized_corpus = [doc.split() for doc in self.corpus]\n",
    "\n",
    "        # Step 3: Initialize the BM25 retriever with the tokenized corpus.\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve the top `k` most relevant documents for a given query.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query as a string.\n",
    "        - k (int): The number of top documents to return (default is 5).\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of the top `k` relevant documents as strings.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize the input query.\n",
    "        # Hint: Use `.split()` to tokenize the query into words.\n",
    "        tokenized_query = query.split()\n",
    "\n",
    "        # Step 2: Use the BM25 retriever to score and rank documents.\n",
    "        # Hint: Use the `bm25.get_top_n()` method to retrieve the top `k` documents.\n",
    "        top_docs = self.bm25.get_top_n(tokenized_query, self.corpus, n=k)\n",
    "\n",
    "        # Step 3: Return the top `k` relevant documents.\n",
    "        return top_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skPWlylvpWgB"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrEkVO9UpUtk"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Initialize the retriever with the sample documents.\n",
    "retriever = BM25Retriever(sample_docs)\n",
    "\n",
    "# Test the retriever with a query.\n",
    "query = \"What is machine learning?\"\n",
    "top_docs = retriever.retrieve(query, k=2)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Top Relevant Documents:\")\n",
    "for idx, doc in enumerate(top_docs, 1):\n",
    "    print(f\"{idx}. {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmdT52BtqwfQ"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Top Relevant Documents:\n",
    "1. Machine learning is a method of data analysis that automates analytical model building.\n",
    "2. Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s42FiJ8rEOE"
   },
   "source": [
    "1. If two documents have identical content except for minor differences (e.g., synonyms or paraphrasing), how might BM25 handle this, and why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. What challenges might arise if the corpus contains very short or very long documents? How would you address these challenges?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZhBH_Aqs2of"
   },
   "source": [
    "# 8. Build Chroma\n",
    "In this task, students will implement a function to build a Chroma vector store, a key component of the RAG pipeline. The Chroma vector store enables efficient semantic search by embedding documents into a high-dimensional vector space. Using these embeddings, the retriever can find documents that are semantically similar to a given query.\n",
    "\n",
    "The task involves:\n",
    "\n",
    "1. Initializing a vector store (Chroma) with Hugging Face embeddings.\n",
    "2. Adding a list of documents to the vector store.\n",
    "3. Returning the vector store for later use in the retrieval and generation pipeline.\n",
    "\n",
    "This function sets up the semantic retrieval system, allowing for more meaningful and context-aware results than keyword-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiAtoavjpgPt"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_chroma(documents: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Build a Chroma vector store using Hugging Face embeddings\n",
    "    and add the documents to it.\n",
    "\n",
    "    Parameters:\n",
    "    - documents (list[Document]): A list of Document objects to add to the vector store.\n",
    "\n",
    "    Returns:\n",
    "    - Chroma: The Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize Hugging Face embeddings.\n",
    "    # - Use a pre-trained embedding model (e.g., \"sentence-transformers/all-mpnet-base-v2\").\n",
    "    # - HuggingFaceEmbeddings generates dense vector representations for text.\n",
    "    embeddings = None  # Replace with your implementation.\n",
    "\n",
    "    # Step 2: Initialize the Chroma vector store.\n",
    "    # - Set the collection name for the vector store (e.g., \"EngGenAI\").\n",
    "    # - Pass the Hugging Face embeddings as the embedding function.\n",
    "    vector_store = None  # Replace with your implementation.\n",
    "\n",
    "    # Step 3: Add the input documents to the Chroma vector store.\n",
    "    # - Use the `add_documents` method to embed and store the documents.\n",
    "    pass  # Replace with your implementation.\n",
    "\n",
    "    # Step 4: Return the Chroma vector store for later use.\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivy_VQFItWTr"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0Vb-uDutN3-"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Call the function to build the Chroma vector store.\n",
    "vector_store = build_chroma(sample_docs)\n",
    "\n",
    "# Test retrieval (optional, if supported).\n",
    "print(\"Vector store built successfully!\")\n",
    "print(vector_store)  # Print the vector store object to verify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TgIqUZGttLE"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Vector store built successfully!\n",
    "\n",
    "<langchain.vectorstores.Chroma object at 0x7f8c1a4b3f10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfDwKvbcvHvN"
   },
   "source": [
    "1. What happens if two documents have identical embeddings? How would you handle this in the retrieval process?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgxRBNaxv-ro"
   },
   "source": [
    "# 9. Ensemble Retriever\n",
    "\n",
    "In this task, students will implement an Ensemble Retriever that combines the strengths of Chroma (semantic similarity) and BM25 (keyword-based retrieval) to create a hybrid retriever. This ensemble approach ensures more robust and comprehensive retrieval results by leveraging both semantic and lexical search techniques.\n",
    "\n",
    "You should:\n",
    "\n",
    "1. Retrieve documents from both Chroma (semantic search) and BM25 (lexical search).\n",
    "2. Combine the results from both retrievers while deduplicating overlapping results.\n",
    "3. Return the top k most relevant and unique documents.\n",
    "\n",
    "This function plays a vital role in the RAG pipeline by ensuring that the retrieved documents are relevant and diverse, combining semantic understanding with precise keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYdmjXWFtxca"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleRetriever with Chroma and BM25 retrievers.\n",
    "\n",
    "        Parameters:\n",
    "        - chroma_store: The Chroma vector store for semantic retrieval.\n",
    "        - bm25_retriever: The BM25 retriever for lexical retrieval.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the Chroma vector store and BM25 retriever.\n",
    "        # Hint: Assign the inputs `chroma_store` and `bm25_retriever` to instance variables.\n",
    "        self.chroma_store = None  # Replace with your implementation.\n",
    "        self.bm25_retriever = None  # Replace with your implementation.\n",
    "\n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input search query.\n",
    "        - k (int): The number of top unique documents to return (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - list[Document]: A list of unique relevant documents.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Retrieve top-k documents from Chroma (semantic similarity).\n",
    "        chroma_docs = None  # Replace with your implementation.\n",
    "\n",
    "        # Step 2: Retrieve top-k documents from BM25 (lexical matching).\n",
    "        bm25_docs = None  # Replace with your implementation.\n",
    "\n",
    "        # Step 3: Combine results from both retrievers into a single list.\n",
    "        combined = None  # Replace with your implementation.\n",
    "\n",
    "        # Step 4: Deduplicate the combined results.\n",
    "        # Hint: Use a `set` to track seen content based on document text.\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in combined:\n",
    "            # Retrieve content for deduplication (check if `page_content` exists).\n",
    "            # Hint: Use `doc.page_content` if it's a Document object; otherwise, use `doc` as is.\n",
    "            content = None  # Replace with your implementation.\n",
    "\n",
    "            # Use the first 60 characters of the document text as a key for deduplication.\n",
    "            key = None  # Replace with your implementation.\n",
    "\n",
    "            if key not in seen:\n",
    "                # Convert plain strings to Document objects if necessary.\n",
    "                # Hint: Use `Document(page_content=doc)` for plain text.\n",
    "                if isinstance(doc, str):\n",
    "                    doc = None  # Replace with your implementation.\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "\n",
    "        # Step 5: Return the top-k unique documents.\n",
    "        return None  # Replace with your implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86StDSBJwzYj"
   },
   "source": [
    "Run the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz8G5gThtxZf"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates model building using data.\"),\n",
    "    Document(page_content=\"Deep learning is a type of machine learning using neural networks.\"),\n",
    "    Document(page_content=\"AI includes technologies like machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing focuses on human-computer language interaction.\"),\n",
    "]\n",
    "\n",
    "# Sample Chroma and BM25 retrievers (mock behavior)\n",
    "class MockChroma:\n",
    "    def similarity_search(self, query, k):\n",
    "        return [Document(page_content=\"Machine learning automates model building using data.\")]\n",
    "\n",
    "class MockBM25:\n",
    "    def retrieve(self, query, k):\n",
    "        return [\"Deep learning is a type of machine learning using neural networks.\"]\n",
    "\n",
    "# Initialize mock retrievers\n",
    "chroma = MockChroma()\n",
    "bm25 = MockBM25()\n",
    "\n",
    "# Initialize EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(chroma, bm25)\n",
    "\n",
    "# Test the retriever with a query\n",
    "query = \"What is machine learning?\"\n",
    "results = ensemble_retriever.get_relevant_documents(query, k=3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Ensemble Retrieval Results:\")\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    print(f\"{idx}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5atP_1k5wmr9"
   },
   "source": [
    "Ensemble Retrieval Results:\n",
    "1. Machine learning automates model building using data.\n",
    "2. Deep learning is a type of machine learning using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp3i6YimxSEl"
   },
   "source": [
    "1. Why is it beneficial to combine semantic retrieval (Chroma) and lexical retrieval (BM25) in an Ensemble Retriever?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If the results from Chroma and BM25 are drastically different (little to no overlap), how might this impact the quality of the combined results?\n",
    "\n",
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lqc0U7qgtwKs"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class StrOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpqUJMc2yzS-"
   },
   "source": [
    "# 10. Format Documents\n",
    "\n",
    "In this task, you will implement two key components to enhance the formatting and styling of documents in the RAG pipeline:\n",
    "\n",
    "format_docs(docs):\n",
    "\n",
    "This function takes a list of documents (docs) and formats them into a readable, numbered list. If no documents are provided, it returns a default message indicating the absence of context.\n",
    "\n",
    "style_prompt:\n",
    "\n",
    "This is a prompt template that prepares the input for a neural style transfer task. It asks an AI model to rewrite a given text (original_text) in a specified style, optionally using a contextual snippet (context) from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "3r_YDxMozMZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format a list of documents into a numbered, readable string.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list[Document]): A list of Document objects to format.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string containing the formatted documents or a default message if no documents are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Check if the list of documents is empty.\n",
    "    # Hint: If `docs` is empty, return the string \"No relevant context found.\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"\n",
    "\n",
    "    # Step 2: Initialize an empty list to store formatted snippets.\n",
    "    snippet_list = []\n",
    "\n",
    "    # Step 3: Iterate over the documents and format each one.\n",
    "    # - Use `enumerate` to get the index and document.\n",
    "    # - Extract and clean the `page_content` of the document.\n",
    "    # - Replace newlines with spaces and remove unnecessary whitespace.\n",
    "    # - Add a formatted string to the `snippet_list` (e.g., \"1. Cleaned content\").\n",
    "    for i, doc in enumerate(docs):\n",
    "        cleaned_content = doc.page_content.replace(\"\\n\", \" \").strip()\n",
    "        snippet_list.append(f\"{i + 1}. {cleaned_content}\")\n",
    "\n",
    "    # Step 4: Join the snippets with newline characters and return the result.\n",
    "    return \"\\n\".join(snippet_list)\n",
    "\n",
    "\n",
    "# Define the style transfer prompt template\n",
    "style_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"context\", \"original_text\"],\n",
    "    template=(\n",
    "        \"Rewrite the following text in the style of {style}.\\n\\n\"\n",
    "        \"Context (if any):\\n{context}\\n\\n\"\n",
    "        \"Original Text:\\n{original_text}\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ58VijEzuPH"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "Ld8RnXL9zmnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Documents:\n",
      "\n",
      "1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n",
      "\n",
      "Generated Prompt for Style Transfer:\n",
      "\n",
      "Rewrite the following text in the style of poetic.\n",
      "\n",
      "Context (if any):\n",
      "1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n",
      "\n",
      "Original Text:\n",
      "Artificial intelligence is transforming the world.\n",
      "\n",
      "\n",
      "--- Rewritten (Styled) Text ---\n",
      "Machine learning is a subset of AI that automates data analysis.\n",
      "Deep learning, a subset of machine learning, uses neural networks to learn patterns.\n",
      "AI includes various technologies, such as natural language processing and computer vision.\n",
      "\n",
      "Rewritten Text:\n",
      "In the realm of the artificial, intelligence, a specter, dances,\n",
      "Transforming the world as we know it, in grand, sweeping, changes.\n",
      "Machine learning, a child of AI, automates the intricate dance,\n",
      "Of data analysis, in a symphony of numbers, vast and expansive.\n",
      "\n",
      "Deep learning, a prodigy of machine learning, wears the crown,\n",
      "Of neural networks, learning patterns, in a learned, profound, gown.\n",
      "Artificial intelligence, the grand master, embraces various arts,\n",
      "Natural language processing, computer vision, and other, beautiful, parts.\n",
      "\n",
      "A dance of numbers, a symphony of words, a specter of light,\n",
      "Artificial intelligence, in its infinite wisdom, takes its right.\n",
      "A transformation, a dance, a specter, a light,\n",
      "Artificial intelligence, in its grand, infinite, might.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # Or the specific LLM library you're using\n",
    "\n",
    "# Example setup for LLM (ensure this is compatible with your LLM)\n",
    "def setup_llm():\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Replace with the appropriate model\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates data analysis.\"),\n",
    "    Document(page_content=\"Deep learning uses neural networks to learn patterns.\"),\n",
    "    Document(page_content=\"Artificial intelligence includes various technologies.\"),\n",
    "]\n",
    "\n",
    "# Test the format_docs function\n",
    "formatted_docs = format_docs(sample_docs)\n",
    "print(\"Formatted Documents:\\n\")\n",
    "print(formatted_docs)\n",
    "\n",
    "# Test the style_prompt with sample inputs\n",
    "style = \"poetic\"\n",
    "context = formatted_docs\n",
    "original_text = \"Artificial intelligence is transforming the world.\"\n",
    "\n",
    "styled_prompt = style_prompt.format(\n",
    "    style=style,\n",
    "    context=context,\n",
    "    original_text=original_text,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Prompt for Style Transfer:\\n\")\n",
    "print(styled_prompt)\n",
    "\n",
    "# Pass the prompt to the LLM\n",
    "llm = setup_llm()  # Initialize the LLM\n",
    "styled_output = llm(styled_prompt)  # Generate the styled text\n",
    "\n",
    "print(\"\\n--- Rewritten (Styled) Text ---\")\n",
    "print(styled_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq3HaHd0CFog"
   },
   "source": [
    "# 11. RAG chain\n",
    "\n",
    "In this task, students will implement a RAG chain that integrates an ensemble retriever (Chroma and BM25), formats retrieved context, applies a prompt template, and generates styled output using a Language Model (LLM).\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "Use the EnsembleRetriever to retrieve relevant documents from Chroma and BM25.\n",
    "Format the retrieved documents into a readable context.\n",
    "Generate a prompt for neural style transfer using the retrieved context and the input query.\n",
    "Pass the prompt to the LLM and parse the model's response to return the final styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nbvH2EZ9zp-d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "\n",
    "def build_rag_chain(llm, chroma_store, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using an ensemble retriever with Chroma and BM25,\n",
    "    followed by formatting the context, applying the prompt, and parsing the output.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: The language model for generating styled text.\n",
    "    - chroma_store: Chroma vector store for semantic retrieval.\n",
    "    - bm25_retriever: BM25 retriever for lexical retrieval.\n",
    "\n",
    "    Returns:\n",
    "    - rag_chain: A function that processes inputs through the RAG pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Define the Ensemble Retriever\n",
    "    class EnsembleRetriever:\n",
    "        def __init__(self, chroma, bm25):\n",
    "            self.chroma = chroma\n",
    "            self.bm25 = bm25\n",
    "\n",
    "        def retrieve(self, query, k=5):\n",
    "            chroma_docs = self.chroma.similarity_search(query, k=k)\n",
    "            bm25_docs = self.bm25.retrieve(query, k=k)\n",
    "            combined_docs = chroma_docs + bm25_docs\n",
    "            return combined_docs[:k]\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(chroma_store, bm25_retriever)\n",
    "\n",
    "    # Step 2: Define a function to retrieve and format context\n",
    "    def retrieve_and_format_context(query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents and format them into a readable context.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query.\n",
    "        - k (int): The number of documents to retrieve (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - str: The formatted context string.\n",
    "        \"\"\"\n",
    "        # Step 2.1: Retrieve relevant documents using the ensemble retriever.\n",
    "        context_docs = ensemble_retriever.retrieve(query, k=k)\n",
    "\n",
    "        # Step 2.2: Format the retrieved documents.\n",
    "        context = format_docs(context_docs)\n",
    "\n",
    "        return context\n",
    "\n",
    "    # Step 3: Define the RAG chain\n",
    "    def rag_chain(inputs):\n",
    "        \"\"\"\n",
    "        Process inputs through the RAG pipeline to generate styled output.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (dict): A dictionary containing:\n",
    "            - \"question\" (str): The query for retrieving context.\n",
    "            - \"style\" (str): The desired writing style.\n",
    "            - \"original_text\" (str): The text to be rewritten.\n",
    "\n",
    "        Returns:\n",
    "        - str: The final styled output.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 3.1: Retrieve and format the context using the helper function.\n",
    "        query = inputs[\"question\"]\n",
    "        context = retrieve_and_format_context(query)\n",
    "\n",
    "        # Step 3.2: Generate the prompt using the `style_prompt`.\n",
    "        prompt = style_prompt.format(\n",
    "            style=inputs[\"style\"],\n",
    "            context=context,\n",
    "            original_text=inputs[\"original_text\"]\n",
    "        )\n",
    "\n",
    "        # Step 3.3: Pass the prompt through the LLM to generate the output.\n",
    "        llm_output = llm(prompt)\n",
    "\n",
    "        # Step 3.4: Parse the LLM's output to extract the final styled text.\n",
    "        parser = RunnablePassthrough()\n",
    "        result = parser.invoke(llm_output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0NBNIFCDgvE"
   },
   "source": [
    "# 12. Final response\n",
    "\n",
    "In this task, students will implement the main script that integrates all components of the RAG pipeline into a complete application. The script will:\n",
    "\n",
    "1. Scrape content from specified URLs, process the raw text, and split it into smaller, retrievable chunks.\n",
    "2. Build the retrievers: Create a Chroma vector store and a BM25 retriever using the processed documents.\n",
    "3. Build the RAG chain: Set up a pipeline that integrates the retrievers, context formatting, and an LLM to perform neural style transfer.\n",
    "4. Run the application: Accept a user query and a target style, then process the input through the RAG chain to produce styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1pFwYzOCZvj"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main script for scraping, building retrievers, setting up the RAG chain,\n",
    "    and running a neural style transfer demo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Scrape content and split into documents\n",
    "    print(\"Step 1: Scraping content and splitting into documents...\")\n",
    "    example_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "\n",
    "    # Step 1A: Initialize an empty list to store all documents\n",
    "    all_docs = []\n",
    "\n",
    "    # Step 1B: Iterate through the URLs to fetch and process content\n",
    "    for url in example_urls:\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "\n",
    "        # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "        raw_text = None  # Replace with your implementation\n",
    "\n",
    "        # Step 1B.2: Split the raw text into chunks (documents)\n",
    "        splits = None  # Replace with your implementation\n",
    "\n",
    "        # Step 1B.3: Add the chunks to the list of documents\n",
    "        all_docs.extend(splits)\n",
    "\n",
    "    print(f\"Total number of documents: {len(all_docs)}\")\n",
    "\n",
    "    # Step 2: Build Chroma and BM25 retrievers\n",
    "    print(\"Step 2: Building Chroma vector store and BM25 retriever...\")\n",
    "\n",
    "    # Step 2A: Build the Chroma vector store\n",
    "    chroma_store = None  # Replace with your implementation\n",
    "\n",
    "    # Step 2B: Build the BM25 retriever\n",
    "    bm25_retriever = None  # Replace with your implementation\n",
    "\n",
    "    # Step 3: Build the RAG chain\n",
    "    print(\"Step 3: Building RAG chain...\")\n",
    "\n",
    "    # Step 3A: Set up the LLM\n",
    "    llm = None  # Replace with your implementation\n",
    "\n",
    "    # Step 3B: Build the RAG chain\n",
    "    rag_chain = None  # Replace with your implementation\n",
    "\n",
    "    # Step 4: Neural Style Transfer Demo\n",
    "    print(\"\\nStep 4: Neural Style Transfer Demo...\")\n",
    "\n",
    "    # Step 4A: Define the user query and target style\n",
    "    user_text = \"Explain machine learning.\"\n",
    "    target_style = \"as if it were a recipe for cooking\"\n",
    "    inputs = {\"question\": user_text, \"style\": target_style, \"original_text\": user_text}\n",
    "\n",
    "    print(\"\\n============================================\")\n",
    "    print(\"        Neural Style Transfer Demo          \")\n",
    "    print(\"============================================\")\n",
    "    print(f\"Original Text : {user_text}\")\n",
    "    print(f\"Desired Style : {target_style}\")\n",
    "\n",
    "    # Step 5: Run the RAG chain\n",
    "    print(\"\\nStep 5: Running the RAG chain...\")\n",
    "\n",
    "    # Hint: Pass `inputs` through the RAG chain to generate styled output.\n",
    "    styled_result = None  # Replace with your implementation\n",
    "\n",
    "    print(\"\\n--- Styled Output ---\")\n",
    "    print(styled_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVYsup1kFTUW"
   },
   "source": [
    "***What You Should Remember:***\n",
    "\n",
    "1. RAG (Retrieval-Augmented Generation) combines the power of information\n",
    "retrieval and language models to generate accurate and context-aware responses.\n",
    "\n",
    "2. Chroma Vector Store is used for semantic retrieval by embedding documents into high-dimensional vectors and finding semantically similar documents for a given query.\n",
    "\n",
    "3. BM25 Retriever uses lexical matching to rank documents based on the occurrence of query terms, ensuring precision in keyword-based searches.\n",
    "\n",
    "4. Ensemble Retriever merges results from Chroma (semantic similarity) and BM25 (lexical matching) to provide a balance of relevance and diversity in retrieved documents.\n",
    "\n",
    "5. Formatting Context ensures that retrieved documents are clean, readable, and useful for the LLM, improving the quality of generated outputs.\n",
    "\n",
    "6. Prompt Templates guide the LLM by structuring inputs, specifying the task (e.g., style transfer), and ensuring clarity and relevance.\n",
    "\n",
    "7. Neural Style Transfer enables the LLM to rewrite text in a specified style (e.g., formal, poetic, conversational) using both the original input and retrieved context.\n",
    "\n",
    "8. Building a RAG pipeline requires:\n",
    "\n",
    "    **Data preparation:** Scraping and splitting raw text into smaller, retrievable chunks.\n",
    "\n",
    "    **Retriever setup:** Combining Chroma and BM25 to maximize retrieval quality.\n",
    "\n",
    "    **Chain integration:** Connecting the retrievers, context formatting, and LLM to form a cohesive workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ1Eo7i5GSDT"
   },
   "source": [
    "Congratulations! You've come to the end of this assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
